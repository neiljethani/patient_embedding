{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import imp\n",
    "import re\n",
    "\n",
    "from mimic3models.patient_embedding import utils\n",
    "from mimic3benchmark.readers import PatientEmbeddingReader, InHospitalMortalityReader\n",
    "\n",
    "from mimic3models.preprocessing import DiscretizerContinuous, Normalizer\n",
    "from mimic3models import common_utils\n",
    "\n",
    "from mimic3models.pytorch_models.embedding.dataset.utils import PatientEmbeddingDataset\n",
    "from mimic3models.pytorch_models.classification.dataset.utils import ClassificationDataset\n",
    "from mimic3models.pytorch_models.embedding.train.train import EmbeddingTrainer\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"/home/neil.jethani/patient_embedding/data/in_hospital_mortality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Discretizer and Normalizer\n"
     ]
    }
   ],
   "source": [
    "# Build readers, discretizers, normalizers\n",
    "\n",
    "train_reader = InHospitalMortalityReader(dataset_dir=os.path.join(data, 'val'), \n",
    "                                          listfile=os.path.join(data, 'val', 'listfile.csv'), \n",
    "                                          period_length=48.0)\n",
    "\n",
    "\n",
    "val_reader = InHospitalMortalityReader(dataset_dir=os.path.join(data, 'val_test'),\n",
    "                                    listfile=os.path.join(data, 'val_test', 'listfile.csv'),\n",
    "                                    period_length=48.0)\n",
    "\n",
    "print(\"Initializing Discretizer and Normalizer\")\n",
    "discretizer = DiscretizerContinuous(timestep=1.0,\n",
    "                                    store_masks=False,\n",
    "                                    impute_strategy='previous',\n",
    "                                    start_time='zero')\n",
    "\n",
    "discretizer_header = discretizer.transform(train_reader.read_example(0)[\"X\"])[1]\n",
    "cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "\n",
    "normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize\n",
    "normalizer_state = None\n",
    "if normalizer_state is None:\n",
    "    normalizer_state = 'ptemb_ts{}.input_str:{}.start_time:zero.normalizer'.format(1.0, 'previous')\n",
    "    normalizer_state = os.path.join(\"/home/neil.jethani/patient_embedding/src/mimic3models/patient_embedding\", normalizer_state)\n",
    "normalizer.load_params(normalizer_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Dataset\n"
     ]
    }
   ],
   "source": [
    "#Create Dataset + DataLoader\n",
    "print(\"Building Dataset\")\n",
    "train_dataset = ClassificationDataset(reader=train_reader, discretizer=discretizer, \n",
    "                                        normalizer=normalizer, return_name=False, \n",
    "                                        embed_method='DAE')\n",
    "val_dataset = ClassificationDataset(reader=val_reader, discretizer=discretizer, \n",
    "                                      normalizer=normalizer, return_name=True, \n",
    "                                      embed_method='DAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(408,)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.113479</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>-2.235415</td>\n",
       "      <td>-1.776871</td>\n",
       "      <td>-2.599113</td>\n",
       "      <td>-1.296513</td>\n",
       "      <td>-0.370500</td>\n",
       "      <td>0.791892</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.167818</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>0.903197</td>\n",
       "      <td>1.002486</td>\n",
       "      <td>0.425492</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.058328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.096025</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>-2.235415</td>\n",
       "      <td>-1.776871</td>\n",
       "      <td>-2.599113</td>\n",
       "      <td>-1.296513</td>\n",
       "      <td>-0.370500</td>\n",
       "      <td>0.791892</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.147034</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>1.694514</td>\n",
       "      <td>0.956628</td>\n",
       "      <td>0.425492</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.096025</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>-0.321417</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.210765</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.370500</td>\n",
       "      <td>1.032938</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.152230</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>0.903197</td>\n",
       "      <td>1.185922</td>\n",
       "      <td>0.095380</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.106498</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>-0.321417</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.210765</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.370500</td>\n",
       "      <td>0.888310</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.167818</td>\n",
       "      <td>-0.000307</td>\n",
       "      <td>0.111880</td>\n",
       "      <td>1.231781</td>\n",
       "      <td>0.095380</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.134425</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>-0.321417</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.210765</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>1.177566</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.198995</td>\n",
       "      <td>-0.000307</td>\n",
       "      <td>0.428406</td>\n",
       "      <td>1.140063</td>\n",
       "      <td>0.095380</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.130934</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>-0.321417</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.210765</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.791892</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.204191</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>0.428406</td>\n",
       "      <td>1.277639</td>\n",
       "      <td>0.095380</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.123952</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.936520</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.214583</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>-0.362911</td>\n",
       "      <td>1.736228</td>\n",
       "      <td>0.323918</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.123952</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.791892</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.193799</td>\n",
       "      <td>-0.001553</td>\n",
       "      <td>3.435411</td>\n",
       "      <td>1.185922</td>\n",
       "      <td>0.323918</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.144897</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.743683</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.235367</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>3.435411</td>\n",
       "      <td>1.644510</td>\n",
       "      <td>0.323918</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.134425</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.888310</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.209387</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>0.270143</td>\n",
       "      <td>1.461075</td>\n",
       "      <td>0.323918</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.137915</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.984729</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.230171</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>-0.362911</td>\n",
       "      <td>1.598651</td>\n",
       "      <td>0.222351</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.096025</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.840101</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.167818</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>-0.046384</td>\n",
       "      <td>1.185922</td>\n",
       "      <td>0.222351</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.137915</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.936520</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.230171</td>\n",
       "      <td>-0.000307</td>\n",
       "      <td>0.586670</td>\n",
       "      <td>1.827946</td>\n",
       "      <td>0.222351</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.137915</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.936520</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.230171</td>\n",
       "      <td>-0.000307</td>\n",
       "      <td>0.586670</td>\n",
       "      <td>1.827946</td>\n",
       "      <td>0.222351</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.347367</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.984729</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.453602</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>-0.046384</td>\n",
       "      <td>3.111993</td>\n",
       "      <td>0.120787</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.183297</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.599055</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.302916</td>\n",
       "      <td>-0.001553</td>\n",
       "      <td>0.586670</td>\n",
       "      <td>2.286534</td>\n",
       "      <td>0.120787</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.193769</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.454427</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.323700</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>0.111880</td>\n",
       "      <td>2.469969</td>\n",
       "      <td>0.120787</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.214714</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.454427</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.365269</td>\n",
       "      <td>-0.000307</td>\n",
       "      <td>0.270143</td>\n",
       "      <td>3.020275</td>\n",
       "      <td>0.120787</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.253114</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.936520</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.406837</td>\n",
       "      <td>-0.000307</td>\n",
       "      <td>0.586670</td>\n",
       "      <td>3.341287</td>\n",
       "      <td>0.095380</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.179806</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.550846</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.297720</td>\n",
       "      <td>-0.000307</td>\n",
       "      <td>0.270143</td>\n",
       "      <td>2.240675</td>\n",
       "      <td>0.095380</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.130934</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.406218</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.209387</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>0.586670</td>\n",
       "      <td>1.369357</td>\n",
       "      <td>0.095380</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.148388</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.550846</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.261347</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>0.586670</td>\n",
       "      <td>2.057240</td>\n",
       "      <td>0.095380</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.183297</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.550846</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.339288</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.428406</td>\n",
       "      <td>2.148957</td>\n",
       "      <td>-0.006189</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.047935</td>\n",
       "      <td>0.183297</td>\n",
       "      <td>-0.454836</td>\n",
       "      <td>0.635583</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.522974</td>\n",
       "      <td>0.840339</td>\n",
       "      <td>-0.792414</td>\n",
       "      <td>0.406218</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>0.323700</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>0.111880</td>\n",
       "      <td>2.653405</td>\n",
       "      <td>-0.006189</td>\n",
       "      <td>-1.629359</td>\n",
       "      <td>0.067352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0  -0.047935  0.113479 -0.454836 -2.235415 -1.776871 -2.599113 -1.296513   \n",
       "1  -0.047935  0.096025 -0.454836 -2.235415 -1.776871 -2.599113 -1.296513   \n",
       "2  -0.047935  0.096025 -0.454836 -0.321417  0.449932  0.210765  0.840339   \n",
       "3  -0.047935  0.106498 -0.454836 -0.321417  0.449932  0.210765  0.840339   \n",
       "4  -0.047935  0.134425 -0.454836 -0.321417  0.449932  0.210765  0.840339   \n",
       "5  -0.047935  0.130934 -0.454836 -0.321417  0.449932  0.210765  0.840339   \n",
       "6  -0.047935  0.123952 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "7  -0.047935  0.123952 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "8  -0.047935  0.144897 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "9  -0.047935  0.134425 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "10 -0.047935  0.137915 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "11 -0.047935  0.096025 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "12 -0.047935  0.137915 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "13 -0.047935  0.137915 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "14 -0.047935  0.347367 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "15 -0.047935  0.183297 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "16 -0.047935  0.193769 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "17 -0.047935  0.214714 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "18 -0.047935  0.253114 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "19 -0.047935  0.179806 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "20 -0.047935  0.130934 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "21 -0.047935  0.148388 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "22 -0.047935  0.183297 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "23 -0.047935  0.183297 -0.454836  0.635583  0.449932  0.522974  0.840339   \n",
       "\n",
       "          7         8         9         10        11        12        13  \\\n",
       "0  -0.370500  0.791892  0.027862  0.167818 -0.000930  0.903197  1.002486   \n",
       "1  -0.370500  0.791892  0.027862  0.147034 -0.000930  1.694514  0.956628   \n",
       "2  -0.370500  1.032938  0.027862  0.152230 -0.000930  0.903197  1.185922   \n",
       "3  -0.370500  0.888310  0.027862  0.167818 -0.000307  0.111880  1.231781   \n",
       "4  -0.792414  1.177566  0.027862  0.198995 -0.000307  0.428406  1.140063   \n",
       "5  -0.792414  0.791892  0.027862  0.204191 -0.000930  0.428406  1.277639   \n",
       "6  -0.792414  0.936520  0.027862  0.214583  0.000316 -0.362911  1.736228   \n",
       "7  -0.792414  0.791892  0.027862  0.193799 -0.001553  3.435411  1.185922   \n",
       "8  -0.792414  0.743683  0.027862  0.235367  0.000316  3.435411  1.644510   \n",
       "9  -0.792414  0.888310  0.027862  0.209387 -0.000930  0.270143  1.461075   \n",
       "10 -0.792414  0.984729  0.027862  0.230171  0.000316 -0.362911  1.598651   \n",
       "11 -0.792414  0.840101  0.027862  0.167818  0.000316 -0.046384  1.185922   \n",
       "12 -0.792414  0.936520  0.027862  0.230171 -0.000307  0.586670  1.827946   \n",
       "13 -0.792414  0.936520  0.027862  0.230171 -0.000307  0.586670  1.827946   \n",
       "14 -0.792414  0.984729  0.027862  0.453602  0.000316 -0.046384  3.111993   \n",
       "15 -0.792414  0.599055  0.027862  0.302916 -0.001553  0.586670  2.286534   \n",
       "16 -0.792414  0.454427  0.027862  0.323700 -0.000930  0.111880  2.469969   \n",
       "17 -0.792414  0.454427  0.027862  0.365269 -0.000307  0.270143  3.020275   \n",
       "18 -0.792414  0.936520  0.027862  0.406837 -0.000307  0.586670  3.341287   \n",
       "19 -0.792414  0.550846  0.027862  0.297720 -0.000307  0.270143  2.240675   \n",
       "20 -0.792414  0.406218  0.027862  0.209387 -0.000930  0.586670  1.369357   \n",
       "21 -0.792414  0.550846  0.027862  0.261347 -0.000930  0.586670  2.057240   \n",
       "22 -0.792414  0.550846  0.027862  0.339288  0.000316  0.428406  2.148957   \n",
       "23 -0.792414  0.406218  0.027862  0.323700 -0.000930  0.111880  2.653405   \n",
       "\n",
       "          14        15        16  \n",
       "0   0.425492 -1.629359  0.058328  \n",
       "1   0.425492 -1.629359  0.067352  \n",
       "2   0.095380 -1.629359  0.067352  \n",
       "3   0.095380 -1.629359  0.067352  \n",
       "4   0.095380 -1.629359  0.067352  \n",
       "5   0.095380 -1.629359  0.067352  \n",
       "6   0.323918 -1.629359  0.067352  \n",
       "7   0.323918 -1.629359  0.067352  \n",
       "8   0.323918 -1.629359  0.067352  \n",
       "9   0.323918 -1.629359  0.067352  \n",
       "10  0.222351 -1.629359  0.067352  \n",
       "11  0.222351 -1.629359  0.067352  \n",
       "12  0.222351 -1.629359  0.067352  \n",
       "13  0.222351 -1.629359  0.067352  \n",
       "14  0.120787 -1.629359  0.067352  \n",
       "15  0.120787 -1.629359  0.067352  \n",
       "16  0.120787 -1.629359  0.067352  \n",
       "17  0.120787 -1.629359  0.067352  \n",
       "18  0.095380 -1.629359  0.067352  \n",
       "19  0.095380 -1.629359  0.067352  \n",
       "20  0.095380 -1.629359  0.067352  \n",
       "21  0.095380 -1.629359  0.067352  \n",
       "22 -0.006189 -1.629359  0.067352  \n",
       "23 -0.006189 -1.629359  0.067352  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6294472507200672"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 0\n",
    "for t in x['src']:\n",
    "    y += sum((t - x['src'][0])**2)\n",
    "    \n",
    "y/(23*17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.503111111111111"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((.011/45)*1024)*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "\n",
    "class Selector(Sampler):\n",
    "    def __init__(self, dataset, mn, mx, bs=1024, indices = None,):\n",
    "        self.min = mn*bs\n",
    "        self.max = mx*bs\n",
    "        self.indices = list(range(len(dataset))) \\\n",
    "            if indices is None else indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in range(self.min, self.max))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max-self.min\n",
    "\n",
    "class Selector2(Sampler):\n",
    "    def __init__(self, dataset, ind, indices=None):\n",
    "        self.indices = list(range(len(dataset))) \\\n",
    "            if indices is None else indices\n",
    "        self.ind = ind\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in range(self.ind, self.ind + 30))\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "299*1024+110+16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building DataLoader\n"
     ]
    }
   ],
   "source": [
    "print(\"Building DataLoader\")\n",
    "trainLoader = DataLoader(train_dataset, batch_size=512, shuffle=False, num_workers=1) #sampler = Selector2(train_dataset,419433))\n",
    "valLoader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=1, sampler = Selector2(train_dataset, 306286))#Selector(train_dataset,290, 300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7915"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader.dataset.get_number_of_visits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Model\n",
    "print(\"Creating Trainer\")\n",
    "trainer = EmbeddingTrainer(model = \"/home/neil.jethani/patient_embedding/models/patient_embedding/DAE/2019-07-22/best/DAE.ep49\", \n",
    "                           output_dir= \"/home/neil.jethani/patient_embedding/models/patient_embedding/DAE/delete\", \n",
    "                           train_dataloader=trainLoader, test_dataloader=valLoader, \n",
    "                           embed_method = \"DAE\", \n",
    "                           layers = 0, heads = 0, MSprop = 0,\n",
    "                           lr = 0.01, betas=(0.9, 0.98), eps=1e-9,\n",
    "                           factor = 2, warmup = 4000, \n",
    "                           log_freq=1, with_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(trainLoader):\n",
    "    x = data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 408])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['X'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.test(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "trainer.dataset_list['test'] = random.shuffle(trainer.dataset_list['test'])\n",
    "trainer.dataset_list['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.dataset_list['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Start\")\n",
    "for epoch in range(args.epochs):\n",
    "    trainer.train(epoch)\n",
    "    trainer.save(epoch)\n",
    "    if valLoader is not None:\n",
    "        trainer.test(epoch)\n",
    "trainer.save_best()\n",
    "trainer.write_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.torch.randn(512, 24, 17)\n",
    "x = F.pad(x, (0,0,1,0,0,0), 'constant', 0)\n",
    "print(x.size())\n",
    "x[1, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,-1, :].unsqueeze(-2).expand(-1, 24, -1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x[:, 1:, :]\n",
    "y.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = x[:, 0, :].unsqueeze(-2).expand(-1, y.size()[1], -1)\n",
    "emb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(3, 10, padding_idx=0)\n",
    "emb(torch.LongTensor([0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.LongTensor(range(1,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(10, 24, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554],\n",
       "         [0.5433, 0.5669, 0.1180, 0.4833, 0.2528, 0.1582, 0.5227, 0.3581,\n",
       "          0.7977, 0.3902, 0.2812, 0.1074, 0.6871, 0.7518, 0.1470, 0.1667,\n",
       "          0.7554]],\n",
       "\n",
       "        [[0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873],\n",
       "         [0.7956, 0.9312, 0.1124, 0.4905, 0.3115, 0.3739, 0.7491, 0.4204,\n",
       "          0.7545, 0.7032, 0.6808, 0.4537, 0.9648, 0.1815, 0.0936, 0.0086,\n",
       "          0.2873]]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 23, :].unsqueeze(1).expand(-1, 24, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7844"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.get_number_of_visits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1432, 0.4390, 0.4101, 0.4472, 0.2437, 0.1816, 0.7163, 0.9704, 0.7701,\n",
       "         0.5479, 0.7096, 0.7371, 0.5672, 0.8374, 0.0588, 0.2799, 0.9094],\n",
       "        [0.1458, 0.9845, 0.3800, 0.9496, 0.6697, 0.9460, 0.0100, 0.3126, 0.4404,\n",
       "         0.4333, 0.8974, 0.6661, 0.0474, 0.0040, 0.2338, 0.6218, 0.7950],\n",
       "        [0.7391, 0.1776, 0.3844, 0.5500, 0.5094, 0.7308, 0.3808, 0.3324, 0.5538,\n",
       "         0.0131, 0.3966, 0.6333, 0.0750, 0.4559, 0.4892, 0.8924, 0.1431],\n",
       "        [0.8294, 0.7924, 0.5776, 0.8835, 0.5446, 0.1719, 0.4953, 0.5111, 0.0217,\n",
       "         0.9746, 0.7972, 0.8229, 0.7079, 0.1877, 0.4452, 0.0889, 0.6590],\n",
       "        [0.5213, 0.3955, 0.4710, 0.5380, 0.6827, 0.5926, 0.5466, 0.1783, 0.4587,\n",
       "         0.5938, 0.0411, 0.3161, 0.9585, 0.1430, 0.8473, 0.0125, 0.6892],\n",
       "        [0.9770, 0.8905, 0.7875, 0.9030, 0.9668, 0.9870, 0.7614, 0.2302, 0.4598,\n",
       "         0.9460, 0.9996, 0.4139, 0.7679, 0.5740, 0.3945, 0.9917, 0.4071],\n",
       "        [0.9779, 0.6108, 0.1203, 0.1470, 0.7982, 0.3797, 0.9532, 0.5090, 0.6162,\n",
       "         0.4747, 0.1559, 0.2526, 0.8807, 0.3285, 0.0564, 0.9295, 0.6196],\n",
       "        [0.7295, 0.7241, 0.5127, 0.4375, 0.4013, 0.1676, 0.3343, 0.1920, 0.8135,\n",
       "         0.2951, 0.0458, 0.3100, 0.5464, 0.1429, 0.4634, 0.2917, 0.6151],\n",
       "        [0.4688, 0.3761, 0.0792, 0.7138, 0.2895, 0.0496, 0.8757, 0.3510, 0.9215,\n",
       "         0.6882, 0.1579, 0.6361, 0.3667, 0.3996, 0.4960, 0.2774, 0.1744],\n",
       "        [0.3944, 0.1524, 0.5105, 0.0570, 0.2940, 0.1486, 0.4614, 0.9461, 0.8031,\n",
       "         0.7147, 0.9589, 0.9620, 0.6186, 0.8435, 0.1848, 0.6003, 0.2729],\n",
       "        [0.3823, 0.1283, 0.9523, 0.7344, 0.6623, 0.9141, 0.4844, 0.4552, 0.1091,\n",
       "         0.0617, 0.0894, 0.4771, 0.4800, 0.4385, 0.8610, 0.1827, 0.4872],\n",
       "        [0.4994, 0.5269, 0.0273, 0.8621, 0.6555, 0.1221, 0.5641, 0.7500, 0.0930,\n",
       "         0.0482, 0.0622, 0.4261, 0.2293, 0.8039, 0.0393, 0.7390, 0.1584],\n",
       "        [0.5417, 0.5130, 0.3754, 0.3461, 0.7191, 0.2700, 0.8382, 0.1779, 0.5571,\n",
       "         0.8470, 0.4486, 0.1798, 0.6875, 0.5432, 0.1823, 0.9007, 0.6396],\n",
       "        [0.3472, 0.0976, 0.8313, 0.1984, 0.9971, 0.8487, 0.3762, 0.1706, 0.5667,\n",
       "         0.7277, 0.2948, 0.1921, 0.0943, 0.9946, 0.3690, 0.0192, 0.9928],\n",
       "        [0.1991, 0.1413, 0.2917, 0.0855, 0.9004, 0.5995, 0.2682, 0.0567, 0.7889,\n",
       "         0.5697, 0.3090, 0.3777, 0.1751, 0.6947, 0.1888, 0.9203, 0.4968],\n",
       "        [0.1378, 0.9074, 0.0957, 0.7534, 0.9557, 0.3988, 0.4652, 0.4108, 0.8645,\n",
       "         0.9501, 0.3481, 0.7552, 0.4719, 0.4326, 0.7862, 0.0957, 0.5371],\n",
       "        [0.3587, 0.5718, 0.8430, 0.4208, 0.4200, 0.1911, 0.3642, 0.3029, 0.1124,\n",
       "         0.6947, 0.8504, 0.9520, 0.1491, 0.5708, 0.9430, 0.3003, 0.5103],\n",
       "        [0.0881, 0.7234, 0.6920, 0.7526, 0.5048, 0.2682, 0.0204, 0.7205, 0.5367,\n",
       "         0.4012, 0.3798, 0.7909, 0.8046, 0.2411, 0.2336, 0.3873, 0.0600],\n",
       "        [0.4225, 0.3344, 0.1618, 0.3357, 0.2845, 0.1741, 0.9600, 0.6302, 0.1687,\n",
       "         0.8406, 0.6593, 0.0991, 0.9444, 0.5375, 0.9018, 0.6466, 0.2909],\n",
       "        [0.1171, 0.3795, 0.0259, 0.5006, 0.4056, 0.2204, 0.4436, 0.2223, 0.6638,\n",
       "         0.0159, 0.8682, 0.4916, 0.7444, 0.7077, 0.5306, 0.0952, 0.8773],\n",
       "        [0.6879, 0.7867, 0.6720, 0.1830, 0.0878, 0.9055, 0.6769, 0.1016, 0.0378,\n",
       "         0.8170, 0.7174, 0.7124, 0.0412, 0.2471, 0.1566, 0.5399, 0.4078],\n",
       "        [0.9138, 0.3080, 0.5588, 0.2996, 0.8609, 0.4815, 0.1335, 0.1967, 0.6753,\n",
       "         0.3566, 0.5294, 0.8222, 0.0362, 0.9572, 0.6979, 0.9032, 0.9707],\n",
       "        [0.9815, 0.3023, 0.2779, 0.5655, 0.5907, 0.4919, 0.6072, 0.7495, 0.2332,\n",
       "         0.5090, 0.7929, 0.4263, 0.1999, 0.0061, 0.5885, 0.9328, 0.7595],\n",
       "        [0.5420, 0.5337, 0.9161, 0.9596, 0.6277, 0.2536, 0.8319, 0.8934, 0.1630,\n",
       "         0.0574, 0.4988, 0.6748, 0.3644, 0.6299, 0.8033, 0.7844, 0.0839],\n",
       "        [0.5061, 0.4916, 0.4564, 0.5260, 0.5863, 0.4373, 0.5237, 0.4321, 0.4762,\n",
       "         0.5241, 0.5003, 0.5470, 0.4566, 0.4884, 0.4563, 0.5181, 0.5232]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((t, t.mean(1).unsqueeze(1)), dim=1)[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.ones(5,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]]\n",
      "[[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.triu(z.numpy(), -1))\n",
    "print(np.triu(z.numpy(), 2))\n",
    "test = np.triu(z.numpy(), -1)\n",
    "test[np.triu(z.numpy(), 2)==1] = 0\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [0. 1. 1. 1. 1.]\n",
      "  [0. 0. 1. 1. 1.]\n",
      "  [0. 0. 0. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [0. 1. 1. 1. 1.]\n",
      "  [0. 0. 1. 1. 1.]\n",
      "  [0. 0. 0. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [0. 1. 1. 1. 1.]\n",
      "  [0. 0. 1. 1. 1.]\n",
      "  [0. 0. 0. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [0. 1. 1. 1. 1.]\n",
      "  [0. 0. 1. 1. 1.]\n",
      "  [0. 0. 0. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]\n",
      "  [0. 1. 1. 1. 1.]\n",
      "  [0. 0. 1. 1. 1.]\n",
      "  [0. 0. 0. 1. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "print(np.triu(z.numpy(), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 5])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:, :-1, :].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 25, 25])\n"
     ]
    }
   ],
   "source": [
    "src_mask = np.ones((1024, 24, 24))\n",
    "    \n",
    "#Add Connections to Neighbors\n",
    "src_mask = np.triu(src_mask, -1)\n",
    "src_mask[np.triu(src_mask, 2)==1] = 0\n",
    "src_mask = torch.tensor(src_mask).float()\n",
    "\n",
    "#Append Connections to Embedding Token\n",
    "src_mask = F.pad(src_mask, (0,1,0,1,0,0), 'constant', 1)\n",
    "print(src_mask.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask[0][10:, 10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
